{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural gradient\n",
    "\n",
    "#### Краткий синопсис\n",
    "\n",
    "В этой части мы рассмотрим тему, которая находится на границе тем о матрице Фишера и KL-дивергенции. Будем использовать оптимизировать по метрике KL (натуральные градиенты) и считать метрики Васерштейна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood оптимизация\n",
    "\n",
    "Очень часто в машинном обучении и статистике возникает задача когда есть данные $X$, некоторое параметрическое распределение $p(x | \\theta)$ и желание найти параметры $\\theta$. \n",
    "\n",
    "Самые лучшие параметры это те которые минимизируют функцию правдоподобия: $\\mathcal{L}(X, \\theta)$, во многих случаях это можно сделать аналитически(почти для всех известных нам именных распределений).\n",
    "\n",
    "В случае если нет возможности посчитать аналитически MLE-оценки приходится обращаться к итеративным методам оптимизации. Один из самых популярных способов это градиентный спуск, который, формально решает следующую задачу:\n",
    "\n",
    "$$d \\theta = \\arg \\max \\mathcal{L}(X|\\theta + d \\theta), s.t. ||d\\theta|| < \\epsilon$$\n",
    "\n",
    "И результатом решения этой задачи является:\n",
    "\n",
    "$$d\\theta = \\frac{\\nabla_\\theta \\mathcal{L}}{||\\nabla_\\theta \\mathcal{L}||} \\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оптимально ли это? \n",
    "\n",
    "Мы ограничиваем наш шаг $||d\\theta||$ сферой радиусом $\\epsilon$ в евклидовом пространстве, но это не особо разумно неразумно. \n",
    "\n",
    "К примеру, сравним два распределения с $\\mu_1 = -2, \\mu_2 = 2$, а дисперсия фиксирована и равна $\\sigma_1 = \\sigma_2 = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import entropy\n",
    "from scipy.stats import norm\n",
    "\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "\n",
    "plt.plot(x, norm.pdf(x, -2, 1))\n",
    "plt.plot(x, norm.pdf(x, 2, 1))\n",
    "plt.axvline(-2, linestyle='--')\n",
    "plt.axvline(2, linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И сравним два распределения с $\\mu_1 = -2, \\mu_2 = 2$, c $\\sigma_1 = \\sigma_2 = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 1000)\n",
    "\n",
    "plt.plot(x, norm.pdf(x, -2, 5))\n",
    "plt.plot(x, norm.pdf(x, 2, 5))\n",
    "plt.axvline(-2, linestyle='--')\n",
    "plt.axvline(2, linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чисто по евклидовой метрики растояние между ними одинаково, но на второй картинке распределения более похожие друг на друга чем на первой.\n",
    "\n",
    "Это можно увидеть если вместо евклидовой метрики смотреть на растояние Кульбака-Лейблера между ними:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(norm.pdf(x, -2, 5), norm.pdf(x, 2, 5)), entropy(norm.pdf(x, -2, 1), norm.pdf(x, 2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По этой причине мы бы хотели научиться решать следующую задачу:\n",
    "\n",
    "$$d \\theta = \\arg \\max \\mathcal{L}(X|\\theta + d \\theta), s.t. KL\\left( p(x| \\theta) || p(x| \\theta + d \\theta) \\right) < \\epsilon$$\n",
    "\n",
    "То есть мы хотели бы на каждом шаге оптимизации делать шаг на $\\epsilon$ по метрике KL, а не по метрике $L_2$.\n",
    "\n",
    "\n",
    "В этом семинаре мы покажем как это делать.\n",
    "\n",
    "Наши шаги:\n",
    "\n",
    "  1. Мы вспомним как выглядит 2D-гауссиана и посчитаем для неё log-likelihood и градиент по log-likelihood.\n",
    "  2. Нам это понадобится чтобы реализовать градиентный спуск.\n",
    "  3. Далее мы вспомним, что такое KL-дивергенция и разложим ёё по Тейлору чтобы найти шаг оптимизации для минимизации KL-дивергенции. \n",
    "  4. Вспомним про связь гессиана KL-дивергенции и матрицы Фишера.\n",
    "  5. Реализуем вот это вот всё."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация данных\n",
    "\n",
    "Сгенерируем данные из распределения 2D-гауссианы с 5 параметрами: $\\vec{\\mu} = [2, -6]$, $\\Sigma = \\begin{pmatrix}\n",
    "5 & 0.8\\\\\n",
    "0.8 & 2\n",
    "\\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_parameters = {\n",
    "    'mean': np.array([2, -6]),\n",
    "    'cov': np.array([[5, 0.8], \n",
    "                     [0.8, 2]])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_decart = np.random.multivariate_normal(**true_parameters, size=(2000,))\n",
    "plt.scatter(X_decart[:, 0], X_decart[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ввод функция для расчёта log-likelihood и pdf.\n",
    "\n",
    "Первым делом, вспомним как выглядят плотность и log-likelihood для 2D-гаусса.\n",
    "\n",
    "\n",
    "$$p(x|\\mu, \\Sigma) = \\frac{1}{2 \\pi |\\Sigma|^{1/2}} \\exp\\left( - \\frac{1}{2} (\\vec{x} - \\vec{\\mu})^T \\Sigma^{-1} (\\vec{x} - \\vec{\\mu}) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_2d_pdf(x: np.ndarray, mean: np.ndarray, cov: np.ndarray):\n",
    "    \"\"\"\n",
    "    pdf нормального распределения в точке x\n",
    "    \"\"\"\n",
    "    assert x.ndim == 1 and mean.ndim == 1\n",
    "    assert len(x) == 2 and len(mean) == 2\n",
    "    <YOUR CODE>\n",
    "    return ll\n",
    "\n",
    "assert np.abs(norm_2d_pdf(np.array([2., -6.]), **true_parameters) - 0.052021420545533555) < 1e-8\n",
    "assert np.abs(norm_2d_pdf(np.array([1., -4.]), **true_parameters) - 0.013538015908347703) < 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_2d_ll(x: np.ndarray, mean: np.ndarray, cov: np.ndarray):\n",
    "    \"\"\"\n",
    "    log-pdf нормального распределения в точке x\n",
    "    \"\"\"\n",
    "    assert x.ndim == 1 and mean.ndim == 1\n",
    "    assert len(x) == 2 and len(mean) == 2\n",
    "    <YOUR CODE>\n",
    "    return ll\n",
    "\n",
    "assert np.abs(norm_2d_ll(np.array([-40., 33.3]), **true_parameters) + 745.0185997116541) < 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_2d_ll_array(X, mean, cov):\n",
    "    ll = 0\n",
    "    for x in X:\n",
    "        ll += norm_2d_ll(x, mean, cov)\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиент 2D-нормального распределения\n",
    "\n",
    "Мы хотим посчитать $\\frac{\\partial \\log p(\\vec{x}|\\vec{\\theta})}{\\partial \\vec{\\theta}} $, где p это наша плотность вероятности, а $\\theta$ - параметры распределения(среднее и матрица ковариации).\n",
    "\n",
    "Подсказки:\n",
    "\n",
    "$$\\frac{\\partial \\log |X|}{\\partial X} = (X^{-1})^T = X^{-T}$$\n",
    "\n",
    "$$\\frac{\\partial \\log a^T X^{-1} b}{\\partial X} = -X^{-T} a b^T X^{-1}$$\n",
    "\n",
    "\n",
    "Книга по всевозможным матричным вычислениям for your interest: \n",
    "http://www.math.uwaterloo.ca/~hwolkowi//matrixcookbook.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_2d_grad(x: np.ndarray, mean: np.ndarray, cov: np.ndarray):\n",
    "    \"\"\"\n",
    "    Градиент от log-pdf по mu и cov\n",
    "    \"\"\"\n",
    "    assert x.ndim == 1 and mean.ndim == 1\n",
    "    assert len(x) == 2 and len(mean) == 2\n",
    "    <YOUR CODE>\n",
    "    return grad_mean, grad_cov\n",
    "\n",
    "grad_mean, grad_cov = norm_2d_grad(np.array([2., -6.]), **true_parameters)\n",
    "assert np.allclose(grad_mean, np.array([0., 0.])) and np.allclose(grad_cov, np.array([[-0.10683761,  0.04273504],\n",
    "                                                                                      [ 0.04273504, -0.26709402]]))\n",
    "\n",
    "grad_mean, grad_cov = norm_2d_grad(np.array([1., -4.]), **true_parameters)\n",
    "assert np.allclose(grad_mean, np.array([-0.38461538,  1.15384615])) and np.allclose(grad_cov, np.array([[-0.03287311, -0.17915845],                                                                                                    [-0.17915845,  0.39858646]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL дивергенция\n",
    "\n",
    "KL-дивергенция выглядит следующим образом:\n",
    "\n",
    "$$KL\\left(p(x|\\theta)||p(x|\\theta')\\right) = \\int p(x|\\theta) \\log \\frac{p(x|\\theta)}{p(x|\\theta')} dx $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь зададимся вопросом: как нам оптимизировать параметры так чтобы спускаться с постоянной скоростью по метрике KL?\n",
    "\n",
    "Для этого расмотрим $\\mathrm{KL}\\left[ p_{\\theta} || p_{\\theta + d \\theta} \\right]$ - KL-дивергенцию от двух очень близких друг к другу распределений и разложим эту функцию по Тейлору при $d \\theta \\rightarrow 0$:\n",
    "\n",
    "$$\n",
    "\\mathrm{KL}\\left[ p_{\\theta} || p_{\\theta + d \\theta} \\right] \\approx \\dots\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL-дивергенция между двумя 2D-нормальными распределениями:\n",
    "\n",
    "\n",
    "$$KL\\left( p(x, \\mu_1, \\Sigma_1) || p(x, \\mu_2, \\Sigma_2) \\right) = \\frac{1}{2} \\left[ \\log \\frac{|\\Sigma_2|}{|\\Sigma_1|} +\n",
    "\\mathrm{Tr} [\\Sigma_2^{-1} \\Sigma_1] + (\\mu_2 - \\mu_1)^T \\Sigma_2^{-1} (\\mu_2 - \\mu_1) - 2 \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'mean': np.array([0., 0.]),\n",
    "    'cov': np.array([[1, 0.], \n",
    "                     [0., 1]])\n",
    "}\n",
    "\n",
    "def mse_difference(parameters_1, parameters_2):\n",
    "    res = (((parameters_1['mean'] - parameters_2['mean'])**2).sum() + \n",
    "          ((parameters_1['cov'] - parameters_2['cov'])**2).sum())\n",
    "    return res / (2 + 4)\n",
    "\n",
    "def kl_difference(parameters_1, parameters_2):\n",
    "    kl = 0\n",
    "    kl += <YOUR CODE>\n",
    "    return kl\n",
    "\n",
    "assert np.abs(kl_difference(parameters, true_parameters) - 11.560530337552443) < 1e-6\n",
    "assert np.abs(kl_difference(true_parameters, parameters) - 21.38177735475525) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Связь KL-дивергенции и матрицы Фишера\n",
    "\n",
    "Гессиан log-likelihood функции это одна из форм записи матрицы Фишера:\n",
    "\n",
    "$$I_n = E \\left[ \\mathrm{H}_{\\log p(x | \\theta)} \\right]$$\n",
    "\n",
    "Таким образом, матрица Фишера, гессиан log-likelihood функции и гессиан KL-дивергенции это одно и тоже.\n",
    "\n",
    "### Расчёт матрицы Фишера\n",
    "\n",
    "Более удобный способ расчёта матрицы Фишера:\n",
    "\n",
    "$$I_n = E\\left[ \\left( \\frac{\\partial \\log p(\\vec{x}|\\vec{\\theta})}{\\partial \\vec{\\theta}} \\right) \n",
    "\\left( \\frac{\\partial \\log p(\\vec{x}|\\vec{\\theta})}{\\partial \\vec{\\theta}}\\right)^T  \\right]$$\n",
    "\n",
    "\n",
    "Производную мы посчитали выше. А как нам посчитать матожидание?\n",
    "\n",
    "Усреднение по элементам выборки есть приближение среднего,поэтому самый простой способ это посчитать матрицу Фишера для каждого элемента нашей выборки и усреднить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Union\n",
    "def empirical_fisher(grad_func: Callable, X: np.ndarray, parameters: dict) -> Union[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Функция принимает на вход матрицу X размера N x 2, функцию расчёта градиента\n",
    "    для среднего и ковариации и параметры нормального распределения.\n",
    "    \n",
    "    На выход возвращается усреднённый по выборке градиент и матрица Фишнера.\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    grads = []\n",
    "    for x in X:\n",
    "        # Посчитать градиент для сэмпла\n",
    "        <YOUR CODE>\n",
    "        # \"Вытянуть\" градиент в один вектор не забыв про условие симметричности матрицы ковариации\n",
    "        <YOUR CODE>\n",
    "        grads.append(grad)\n",
    "    grads = np.vstack(grads)\n",
    "    fisher = <YOUR CODE>\n",
    "    return grads.mean(axis=0), fisher\n",
    "\n",
    "grad, fisher = empirical_fisher(grad_func=norm_2d_grad, X=np.array([[1., 0.], [0., 1.], [1.7, -2.7]]), parameters=true_parameters)\n",
    "\n",
    "\n",
    "assert np.allclose(grad, np.array([-0.6994302 ,  2.99643875,  0.17642207, -2.25459919,  4.61902881]))\n",
    "assert np.allclose(fisher, np.array([[ 0.11597511, -0.36640429, -0.07877717,  0.57713526, -1.03771913],\n",
    "                                     [-0.36640429,  1.19040075,  0.24294858, -1.81013248,  3.3234273 ],\n",
    "                                     [-0.07877717,  0.24294858,  0.05458379, -0.39441812,  0.69675323],\n",
    "                                     [ 0.57713526, -1.81013248, -0.39441812,  2.87737592, -5.14595728],\n",
    "                                     [-1.03771913,  3.3234273 ,  0.69675323, -5.14595728,  9.34679459]]))\n",
    "\n",
    "def vec_to_covariance(vec):\n",
    "    m = np.zeros((2, 2))\n",
    "    m.ravel()[[0, 1, 3]] = vec\n",
    "    m[1, 0] = m[0, 1]\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'mean': np.array([0., 0.]),\n",
    "    'cov': np.array([[1, 0.], \n",
    "                     [0., 1]])\n",
    "}\n",
    "lr = 1e-2\n",
    "sgd_mse = []\n",
    "sgd_mse.append(mse_difference(true_parameters, parameters))\n",
    "\n",
    "sgd_kl = []\n",
    "sgd_kl.append(kl_difference(true_parameters, parameters))\n",
    "\n",
    "sgd_mle = []\n",
    "sgd_mle.append(norm_2d_ll_array(X_decart, **parameters))\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    grad_mean_total = np.zeros(shape=2)\n",
    "    grad_cov_total = np.zeros(shape=(2, 2))\n",
    "    for x in X_decart:\n",
    "        grad_mean, grad_cov = norm_2d_grad(x, **parameters)\n",
    "        grad_mean_total += grad_mean / len(X_decart)\n",
    "        grad_cov_total += grad_cov / len(X_decart)\n",
    "        \n",
    "    parameters['mean'] += lr * grad_mean_total\n",
    "    parameters['cov'] += lr * grad_cov_total\n",
    "    \n",
    "    sgd_mse.append(mse_difference(true_parameters, parameters))\n",
    "    sgd_kl.append(kl_difference(true_parameters, parameters))\n",
    "    sgd_mle.append(norm_2d_ll_array(X_decart, **parameters))\n",
    "\n",
    "    \n",
    "print('mean', parameters['mean'], true_parameters['mean'])\n",
    "print('cov', parameters['cov'], true_parameters['cov'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'mean': np.array([0., 0.]),\n",
    "    'cov': np.array([[1, 0.], \n",
    "                     [0., 1]])\n",
    "}\n",
    "\n",
    "lr = 1e-2\n",
    "nat_grad_mse = []\n",
    "nat_grad_mse.append(mse_difference(true_parameters, parameters))\n",
    "\n",
    "nat_grad_kl = []\n",
    "nat_grad_kl.append(kl_difference(true_parameters, parameters))\n",
    "\n",
    "nat_grad_mle = []\n",
    "nat_grad_mle.append(norm_2d_ll_array(X_decart, **parameters))\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    grad_mean_total = np.zeros(shape=2)\n",
    "    grad_cov_total = np.zeros(shape=(2, 2))\n",
    "    grad, fisher = empirical_fisher(norm_2d_grad, X=X_decart, parameters=parameters)\n",
    "    natural_grad = np.linalg.inv(fisher).dot(grad)\n",
    "    # natural_grad = natural_grad / np.linalg.norm(natural_grad)\n",
    "    \n",
    "    parameters['mean'] += lr * natural_grad[:2]\n",
    "    parameters['cov'] += lr * vec_to_covariance(natural_grad[2:])\n",
    "    \n",
    "    nat_grad_mse.append(mse_difference(true_parameters, parameters))\n",
    "    nat_grad_kl.append(kl_difference(parameters, true_parameters))\n",
    "    nat_grad_mle.append(norm_2d_ll_array(X_decart, **parameters))\n",
    "\n",
    "    \n",
    "print('mean', parameters['mean'], true_parameters['mean'])\n",
    "print('cov', parameters['cov'], true_parameters['cov'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(nat_grad_mse, label='Natural gradient')\n",
    "plt.plot(sgd_mse, label='SGD')\n",
    "plt.title('MSE сравнение')\n",
    "plt.xlabel('iters')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(nat_grad_kl, label='Natural gradient')\n",
    "plt.plot(sgd_kl, label='SGD')\n",
    "plt.title('KL сравнение')\n",
    "plt.xlabel('iters')\n",
    "plt.ylabel('KL')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(nat_grad_mle, label='Natural gradient')\n",
    "plt.plot(sgd_mle, label='SGD')\n",
    "plt.title('MLE сравнение')\n",
    "plt.xlabel('iters')\n",
    "plt.ylabel('KL')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительное задание\n",
    "\n",
    "Выше мы считали матрицу Фишера эмпирическим путём. Это нормальный способ если выборка большая или если интегрировать по плотности распределения сложно, к примеру, если плотность распределения задаётся нейронной сетью.\n",
    "\n",
    "Для нормального распределения матрицу Фишера не сложно посчитать аналитическим путём. \n",
    "\n",
    "Напишите свой код для аналитического расчёта матрицы Фишера в функции ниже(ничего кроме параметров распределения вам не понадобится).\n",
    "\n",
    "Запустите код и скажите что изменилось?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_fisher(parameters: dict) -> Union[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Функция принимает на вход матрицу X размера N x 2, функцию расчёта градиента\n",
    "    для среднего и ковариации и параметры нормального распределения.\n",
    "    \n",
    "    На выход возвращается усреднённый по выборке градиент и матрица Фишнера.\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    grads = []\n",
    "    for x in X:\n",
    "        grad_mean, grad_cov = grad_func(x, **parameters)\n",
    "        grad = np.concatenate([grad_mean, grad_cov.ravel()[[0, 1, 3]]])\n",
    "        grads.append(grad)\n",
    "    \n",
    "    \n",
    "    fisher = <YOUR CODE>\n",
    "    raise ValueError('Where is Fisher, dude?')\n",
    "    \n",
    "    return np.vstack(grads).mean(axis=0), fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'mean': np.array([0., 0.]),\n",
    "    'cov': np.array([[1, 0.], \n",
    "                     [0., 1]])\n",
    "}\n",
    "lr = 1e-2\n",
    "nat_grad_mse = []\n",
    "nat_grad_mse.append(mse_difference(true_parameters, parameters))\n",
    "\n",
    "nat_grad_kl = []\n",
    "nat_grad_kl.append(kl_difference(true_parameters, parameters))\n",
    "\n",
    "for i in tqdm(range(200)):\n",
    "    grad_mean_total = np.zeros(shape=2)\n",
    "    grad_cov_total = np.zeros(shape=(2, 2))\n",
    "    grad, fisher = analytical_fisher(norm_2d_grad, X=X_decart, parameters=parameters)\n",
    "    natural_grad = np.linalg.inv(fisher).dot(grad)\n",
    "    \n",
    "    parameters['mean'] += lr * natural_grad[:2]\n",
    "    parameters['cov'] += lr * vec_to_covariance(natural_grad[2:])\n",
    "    \n",
    "    nat_grad_mse.append(mse_difference(true_parameters, parameters))\n",
    "    nat_grad_kl.append(kl_difference(parameters, true_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(nat_grad_kl, label='Natural gradient')\n",
    "plt.plot(sgd_kl, label='SGD')\n",
    "plt.title('Comparison')\n",
    "plt.xlabel('iters')\n",
    "plt.ylabel('KL')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wasserstein distance\n",
    "\n",
    "Общая формула для расстояния Васерштейна:\n",
    "\n",
    "$$W_p(p, q) = \\inf\\limits_{\\pi \\in \\Gamma(u, v)} \\int\\limits_{\\mathbb{M} \\times \\mathbb{M}} d(x, y)^p d \\pi(x, y)$$\n",
    "\n",
    "\n",
    "Но мы будем расматривать самый простой случай: 1D-растояние с $p=1$.\n",
    "\n",
    "В таком случае формула упрощается до:\n",
    "\n",
    "$$W_1(p, q) = \\inf\\limits_{\\pi \\in \\Gamma(u, v)}  \\int\\limits_{\\mathbb{R} \\times \\mathbb{R}} |x - y| \\pi(x, y)$$\n",
    "\n",
    "Физический смысл: как сделать наименьшее количество работы чтобы перенести лопатой землю из одной кучки в другую:\n",
    "\n",
    "![](https://sbl.inria.fr/fig/Earth_mover_distance_logo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "from itertools import permutations\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_distance_brute_force(x, y):\n",
    "    assert len(x) == len(y)\n",
    "    min_distance = np.inf\n",
    "    argmin_permutation = None\n",
    "    for permutation in permutations(np.arange(len(x))):\n",
    "        <YOUR CODE>\n",
    "    return min_distance\n",
    "\n",
    "\n",
    "x = np.random.randn(4)\n",
    "y = np.random.randn(4)\n",
    "assert np.abs(wasserstein_distance_brute_force(x, y) - wasserstein_distance(x, y)) < 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_distance_smart_force(x, y):\n",
    "    x_idx = np.argsort(x)\n",
    "    y_idx = np.argsort(y)\n",
    "    \n",
    "    distance = <YOUR CODE>\n",
    "    return distance\n",
    "\n",
    "wasserstein_distance_brute_force(x, y) == wasserstein_distance_smart_force(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "times_brute = []\n",
    "times_smart = []\n",
    "Ns = np.arange(2, 10)\n",
    "for i in Ns:\n",
    "    x = np.random.randn(i)\n",
    "    y = np.random.randn(i)\n",
    "    t0 = time.time()\n",
    "    wasserstein_distance_brute_force(x, y)\n",
    "    t1 = time.time()\n",
    "    times_brute.append(t1 - t0)\n",
    "    t0 = time.time()\n",
    "    wasserstein_distance_smart_force(x, y)\n",
    "    t1 = time.time()\n",
    "    times_smart.append(t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(Ns, times_brute, label='brute force')\n",
    "plt.plot(Ns, times_smart, label='smart force')\n",
    "plt.title('Сравнение двух функций по производительности')\n",
    "plt.xlabel('N точек')\n",
    "plt.ylabel('секунды')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "x = np.random.randn(N) - 3\n",
    "y = np.random.randn(N) + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, push_notebook, show\n",
    "from bokeh.models import Arrow, OpenHead, NormalHead, VeeHead\n",
    "from bokeh.models import CustomJS, Slider\n",
    "from bokeh.layouts import row\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.stats import wasserstein_distance\n",
    "def KL(P, Q):\n",
    "    epsilon = 0.00001\n",
    "\n",
    "    Pp = P+epsilon\n",
    "    Pp /= Pp.sum()\n",
    "    Qq = Q+epsilon\n",
    "    Qq /= Qq.sum()\n",
    "\n",
    "    divergence = np.sum(Pp*np.log(Pp/Qq))\n",
    "    return divergence\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "bw = 0.1\n",
    "\n",
    "x_linspace = np.linspace(-10, 10, 1000)\n",
    "x_idx = np.argsort(x)\n",
    "y_idx = np.argsort(y)\n",
    "i = 0\n",
    "density_x = gaussian_kde(x, bw_method=bw)\n",
    "density_y = gaussian_kde(y, bw_method=bw)\n",
    "fig = figure()\n",
    "fig.title.text = \"Wasserstain distance={}, KL={}\".format( wasserstein_distance_smart_force(x, y), KL(density_x(x_linspace),\n",
    "                                                                                                     density_y(x_linspace)))\n",
    "density_x_bokeh = fig.line(x_linspace, density_x(x_linspace), color=\"red\")\n",
    "density_y_bokeh = fig.line(x_linspace, density_y(x_linspace), color=\"blue\")\n",
    "arrow = fig.line([x[x_idx[i]], y[x_idx[i]]], [density_x(x[x_idx[i]])[0], density_y(y[x_idx[i]])[0]], color='green')\n",
    "\n",
    "\n",
    "def update_plot(n_moved):\n",
    "    x_linspace = density_x_bokeh.data_source.data['x']\n",
    "    global density_x\n",
    "    global density_y\n",
    "    x_copy = x.copy()\n",
    "    for i in range(n_moved):\n",
    "        x_copy[x_idx[i]] = y[y_idx[i]]\n",
    "    density_x = gaussian_kde(x_copy, bw_method=bw)\n",
    "    density_x_bokeh.data_source.data['y'] = density_x(x_linspace)\n",
    "    fig.title.text = \"Wasserstain distance={}, KL={}\".format(wasserstein_distance_smart_force(x_copy, y), KL(density_x(x_linspace),\n",
    "                                                                                                             density_y(x_linspace)))\n",
    "    arrow.data_source.data['x'] = [x[x_idx[i + 1]], y[y_idx[i + 1]]]\n",
    "    arrow.data_source.data['y'] = [density_x(x[x_idx[i + 1]])[0], density_y(y[y_idx[i + 1]])[0]]\n",
    "\n",
    "    push_notebook(handle=bokeh_handle)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = CustomJS(code=\"\"\"\n",
    "if (IPython.notebook.kernel !== undefined) {\n",
    "    var kernel = IPython.notebook.kernel;\n",
    "    cmd = \"update_plot(\" + cb_obj.value + \")\";\n",
    "    kernel.execute(cmd, {}, {});\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "slider = Slider(start=0, \n",
    "                end=len(x),\n",
    "                value=0,\n",
    "                step=1,\n",
    "                title=\"Moved dots\",\n",
    "                callback=callback)\n",
    "bokeh_handle = show(row(fig, slider), notebook_handle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
