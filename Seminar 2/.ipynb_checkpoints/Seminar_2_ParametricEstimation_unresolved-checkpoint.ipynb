{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Структура семинара:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Параметрическое оценивание (на примере функции оценки максимального правдоподобия)\n",
    "2. Дельта-метод\n",
    "3. Информация Фишера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Параметрическое оцениваение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам нужно оценить какой-либо параметр из модели. Как правило, мы выбираем модель - скажем, линейную регрессию - и используем наблюдаемые данные $X$ для создания параметров модели $\\theta$.\n",
    "\n",
    "Например, если мы отбираем случайную переменную $\\theta$, которую, как предполагается, обычно распределяем на некоторые средние $\\mu$ и $\\sigma$. Наша цель - найти оценки $\\mu$ и $\\sigma$ из нашего сэмпла, которые точно представляют истинный $\\theta$, а не только образцы, которые мы вытащили.\n",
    "\n",
    "Рассмотрим, когда вы делаете линейную регрессию, и ваша модель оценивает коэффициенты для $\\mu$ на зависимой переменной $y$. Как мы можем максимизировать вероятность нашей оценки $\\theta$ от истинного $X$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция плотности вероятности нормального распределения:\n",
    "![norm](https://cdn-images-1.medium.com/max/1200/1*P78bMZPhhKnzLkwcNgeJ0g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это уравнение говорит нам о вероятности, что $x$ сэмплирован из $p$, когда истинными параметрами распределения являются $\\mu$ и $\\sigma$.\n",
    "\n",
    "Предположим, что наш образец равен $3$, какова вероятность того, что это произойдет из распределения $\\mu=3$ и $\\sigma=1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.pdf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# распределения  μ=3  и  σ=1\n",
    "<YOUR_CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверить распределение с μ = 7 и σ = 2\n",
    "<YOUR_CODE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что у нас есть набор сэмплов $X$, которые происходят из некоторого нормального распределения, и все они взаимно независимы друг от друга. \n",
    "\n",
    "В таком случае, вероятность наблюдения такого набора сэмплов равна произведению вероятностей получить каждый из сэмплов. Это называется **правдоподобием выборки**.\n",
    "\n",
    "Вероятность выбора 2 и 6 из распределения с $\\mu=4$ и $\\sigma=1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR_CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = <YOUR_CODE># [ 2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]\n",
    "x=np.linspace(0,20,100)\n",
    "y=norm.pdf(x, 5, 3)\n",
    "y1=norm.pdf(x, 7, 2)\n",
    "\n",
    "sns.set()\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(x,y)\n",
    "plt.plot(x,y1)\n",
    "plt.scatter(sample, np.random.randn(len(sample)) / 200, c='r')\n",
    "plt.legend(['norm.pdf(x, 5, 3)','norm.pdf(x, 7, 2)'], loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Maximum Likelihood Estimation (MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы хотим найти $p(4, 5, 7, 8, 8, 9, 10, 5, 2, 3, 5, 4, 8, 9; \\mu, \\sigma)$. Вероятность оценивается по нормальному распределению с $\\mu$ и $\\sigma$. Но так как мы не знаем $\\mu$ и $\\sigma$, то нам необходимо оценить их.\n",
    "\n",
    "Мы делаем это с помощью оценки максимального правдоподобия (MLE). MLE основывается на максимизации вероятности пронаблюдать выборку.\n",
    "\n",
    "Наше $\\theta$ является параметром, который оценивает $x = [4, 5, 7, 8, 8, 9, 10, 5, 2, 3, 5, 4, 8, 9]$, который, как мы предполагаем, исходит из нормального распределения PDF, показанного ниже.\n",
    "\n",
    "Мы хотим максимизировать вероятность того, что наш параметр $\\theta$ относится к нормальному распределению. \n",
    "\n",
    "Уравнение правдоподобия чаще всего записывается с использованием логарифма вероятности, так как в этом случае произведение переходит в сумму, что сильно упрощает жизнь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log Likelihood Equation:\n",
    "    \n",
    "![LLE](https://cdn-images-1.medium.com/max/1200/1*H4hW2Haijx9lMdIQJWP32w.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR_CODE> # mean and std from sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the likelihood of the random samples to the two distributions\n",
    "def compare_data_to_dist(x, mu_1=5, mu_2=7, sd_1=3, sd_2=2):\n",
    "    ll_1 = 0\n",
    "    ll_2 = 0\n",
    "    for i in x:\n",
    "        ll_1 += <YOUR_CODE> #log likelihood for mu_1, sd_1\n",
    "        ll_2 += <YOUR_CODE> #log likelihood for mu_2, sd_2\n",
    "    \n",
    "    print (\"The LL of of x for mu = %d and sd = %d is: %.4f\" % (mu_1, sd_1, ll_1))\n",
    "    print (\"The LL of of x for mu = %d and sd = %d is: %.4f\" % (mu_2, sd_2, ll_2))\n",
    "compare_data_to_dist(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы хотим построить логарифмическую вероятность для возможных значений $\\mu$ и $\\sigma$. Ниже мы зафиксировали $\\sigma = 3$, а наше предположение для $\\mu \\in \\{ \\mu \\in R | 0 \\leq x \\leq 20 \\}$. Построим же зависимость $L$ от $\\mu$.\n",
    "\n",
    "Мы можем видеть, что максимум нашей функции правдоподобия встречается вокруг значения [6.2]. Наша цель - найти значения $\\mu$ и $\\sigma$, которые максимизируют функцию правдоподобия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu=np.linspace(0,20,100)\n",
    "x=np.linspace(2,10,100)\n",
    "y=<YOUR_CODE># sd fixed on 3\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(mu,<YOUR_CODE>)\n",
    "plt.legend(['np.log(norm.pdf(7,mu,3))'],loc='upper right')\n",
    "plt.axvline(x=3.7,linewidth=0.5, color='r')\n",
    "plt.title(\"Maximum Likelihood Function\")\n",
    "plt.xlabel(\"Mean Estimate\")\n",
    "plt.ylabel(\"Log Likelihood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдём $\\theta_\\mu$ и $\\theta_\\sigma$ для нормального распределения из максимума правдоподобия.\n",
    "\n",
    "Чтобы максимизировать наше уравнение по каждому из наших параметров, нам нужно взять производную и приравнять к 0.\n",
    "\n",
    "Сначала давайте оценим $\\theta_\\mu$ из нашего уравнения правдоподобия, приведенного выше:\n",
    "\n",
    "$$\\frac{\\delta L}{\\delta \\mu} = 0 \\Rightarrow \\theta_\\mu = \\frac{\\sum x_i}{n}$$\n",
    "\n",
    "$$\\frac{\\delta L}{\\delta \\sigma} = 0 \\Rightarrow \\theta_\\sigma = \\frac{\\sum (x_i - \\theta_\\mu)^2}{n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Maximum Likelihood Functions for different values of mu and sigma\n",
    "def plot_ll(x):\n",
    "    plt.figure(figsize=(5,8))\n",
    "    plt.title(\"Maximim Likelihood Functions\")\n",
    "    plt.xlabel(\"Mean Estimate\")\n",
    "    plt.ylabel(\"Log Likelihood\")\n",
    "    mu_set = np.linspace(0, 16, 1000)\n",
    "    sd_set = [.5, 1, 1.5, 2.5, 3, 3.5]\n",
    "    max_val = max_val_location = None\n",
    "    for i in sd_set:\n",
    "        ll_array = []\n",
    "        \n",
    "        for j in mu_set:\n",
    "            temp_mm = 0\n",
    "            \n",
    "            for k in x:\n",
    "                temp_mm += <YOUR_CODE> # The LL function\n",
    "            ll_array.append(temp_mm)\n",
    "        \n",
    "            if (max_val is None):# what do you expect from it to be plotted?\n",
    "                max_val = max(ll_array)\n",
    "                \n",
    "            elif max(ll_array) > max_val:\n",
    "                max_val = max(ll_array)\n",
    "                max_val_location = j\n",
    "        \n",
    "        # Plot the results\n",
    "        plt.plot(mu_set, ll_array, label=\"sd: %.1f\" % i)    \n",
    "        print (\"The max LL for sd %.2f is %.2f\" % (i, max(ll_array)))\n",
    "        plt.axvline(x=max_val_location, color='black', ls='-.')\n",
    "        plt.legend(loc='lower left')\n",
    "\n",
    "x=np.linspace(2,10,9)\n",
    "plot_ll(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно, что максимальное значение функции правдоподобия реализуется при $\\theta_\\sigma \\approx 2.5$.\n",
    "![Очевидно](http://zloy-tony.ru/wp-content/uploads/2017/03/2C09FFeJO4o.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждая строка строит зависимость максимума правдоподобия от $\\mu$ при некотором фиксированном $\\theta_\\sigma$. Максимум каждой кривой близок к $6.2$. Теперь не сложно увидеть, что максимальное значение функции правдоподобия реализуется при $\\theta_\\sigma \\approx 2.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем для оценки среднего:\n",
    "\n",
    "$$θ_{\\mu} = \\frac{\\sum x_i}{n} = (2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10) / 9 = 6$$\n",
    "\n",
    "Для оценки дисперсии:\n",
    "\n",
    "$$θ_{\\sigma} = \\frac{\\sum (x_i - \\theta_\\mu)^2}{n} = \\frac{\\sum (x_i - 6)^2}{n} = 2.582$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# И вот ещё раз с помощью стандартных функций :)\n",
    "print(np.mean(x))\n",
    "print(np.std(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда у нас есть оценки для $\\mu$ и $\\sigma$  - красный цвет на графике - и посмотрите, как она складывается с потенциальными распределениями, на которые мы смотрели раньше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.linspace(2,10,9)\n",
    "x=np.linspace(0,20,100)\n",
    "y=norm.pdf(x, 5, 3)\n",
    "y1=norm.pdf(x, 7, 2)\n",
    "y_estimated=n<YOUR_CODE>\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(x,y)\n",
    "plt.plot(x,y1)\n",
    "plt.plot(x,y_estimated)\n",
    "plt.scatter(sample, np.random.randn(len(sample)) / 200, c='r')\n",
    "plt.legend(['norm.pdf(x, 5, 3)','norm.pdf(x, 7, 2)','norm.pdf(x, 6, 2.58)'],loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 MAP a.k.a. Maximum a posteriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что вы подбросили моменту один раз и у вас выпал орёл. \n",
    "\n",
    "Распределение описывается распредлением Бернулли: $p(x=1, q) = q$.\n",
    "\n",
    "Для выборки $x_1, x_2, \\dots, x_N$ правдоподобие записывается следующим образом:\n",
    "\n",
    "$$p(X, q) = \\prod q^{x_i} (1 - q)^{1-x_i}$$\n",
    "\n",
    "$$\\log p(X, q) = \\sum\\left[ x_i \\log q + (1 - x_i) \\log( - q) \\right]$$\n",
    "\n",
    "$$\\frac{\\delta }{\\delta q} \\log p(X, q) = \\frac{1}{q} \\sum x_i - \\frac{1}{1-q} \\sum (1 - x_i) = 0$$\n",
    "\n",
    "Получаем:\n",
    "\n",
    "$$q = \\frac{\\sum x_i}{n}$$\n",
    "\n",
    "В согласии с методом максимума правдоподобия следует, что $p=1$, т.е. следует что монетка всегда будет выпадать орлом.\n",
    "\n",
    "Такая оценка не очень хорошо согласуется с реальностью. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако у вас есть некоторое априорное знание. Вы точно знаете, что честная монетка выпадает орлом в 50% случаев.\n",
    "\n",
    "Можем ли мы как-то это использовать? Да, это обеспечивается введением априорного распределения.\n",
    "\n",
    "В MLE оценка параметров выглядела так:\n",
    "\n",
    "$$\\theta = \\mathrm{argmax} \\log P(X | \\theta) = \\mathrm{argmax} \\sum \\log p(x_i, \\theta) $$\n",
    "\n",
    "\n",
    "В MAP мы говорим, что есть ещё распределение $P(\\theta)$, которое появляется из некоторых наших знаний о мире. Тогда MAP записывается следующим образом:\n",
    "\n",
    "$$\n",
    "\\theta = \\mathrm{argmax}  P(X | \\theta) P(\\theta) = \\mathrm{argmax} \\left( \\sum  \\log p(x_i, \\theta) \\right) P(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернёмся к монете.\n",
    "\n",
    "Априорное распределени моделируется бета-распределением у которого плотность выглядит так:\n",
    "\n",
    "![betapdf](https://wikimedia.org/api/rest_v1/media/math/render/svg/125fdaa41844a8703d1a8610ac00fbf3edacc8e7)\n",
    "\n",
    "![beta](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Beta_distribution_pdf.svg/531px-Beta_distribution_pdf.svg.png)\n",
    "\n",
    "В таком случае:\n",
    "\n",
    "$$p(X, q) p(q) = \\prod q^{x_i} (1 - q)^{1-x_i} \\frac{1}{B(\\alpha, \\beta)} q^{\\alpha - 1} (1 - q)^{\\beta - 1}$$\n",
    "\n",
    "Лог-вероятность:\n",
    "\n",
    "$$\\log p(X, q) p(q) = (\\alpha - 1) \\log q + (\\beta - 1) \\log (1 - q) +  \\sum\\left[ x_i \\log q + (1 - x_i) \\log( - q) \\right] $$\n",
    "\n",
    "Производная:\n",
    "\n",
    "\n",
    "$$\\frac{\\delta }{\\delta q} \\log p(X, q) p(q) = $$\n",
    "\n",
    "$$ = \\frac{1}{q} \\sum x_i - \\frac{1}{1-q} \\sum (1 - x_i) + \\frac{\\alpha - 1}{q} - \\frac{\\beta - 1}{1 - q} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решая уравнение выше получаем:\n",
    "\n",
    "$$\\mu = \\frac{\\sum x_i + \\alpha - 1}{n + \\beta + \\alpha - 2}$$\n",
    "\n",
    "\n",
    "Пусть наш приор $\\alpha=\\beta=2$(см. картинку выше).\n",
    "\n",
    "Тогда при одном броске монеты получаем следующую оценку:\n",
    "\n",
    "$$q = \\frac{1 + 2 - 1}{1 + 2 + 2 - 2} = \\frac{2}{3} \\approx 0.66$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь без вывода посмотрим что получится, если тоже самое применить к нормальному распределению на примере из прошлой части. Наш приор, что $\\mu \\sim N(0, 1) = N(\\mu_0, \\sigma_\\mu)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Maximum Likelihood Functions for different values of mu and sigma\n",
    "def plot_ll(x):\n",
    "    plt.figure(figsize=(5,8))\n",
    "    plt.title(\"Maximim Likelihood Functions\")\n",
    "    plt.xlabel(\"Mean Estimate\")\n",
    "    plt.ylabel(\"Log Likelihood\")\n",
    "    mu_set = np.linspace(0, 16, 1000)\n",
    "    sd_set = [.5, 1, 1.5, 2.5, 3, 3.5, 4, 4.5]\n",
    "    max_val = max_val_location = None\n",
    "    # для \n",
    "    for i in sd_set:\n",
    "        ll_array = []\n",
    "        # для \n",
    "        for j in mu_set:\n",
    "            temp_mm = 0\n",
    "            # для измерений\n",
    "            for k in x:\n",
    "                temp_mm += np.log(norm.pdf(k, j, i)) # The LL function\n",
    "            ll_array.append(temp_mm + np.log(norm.pdf(j, 0, 1)))\n",
    "        \n",
    "            if (max_val is None):\n",
    "                max_val = max(ll_array)\n",
    "            elif max(ll_array) > max_val:\n",
    "                max_val = max(ll_array)\n",
    "                max_val_location = j\n",
    "        \n",
    "        # Plot the results\n",
    "        plt.plot(mu_set, ll_array, label=\"sd: %.1f\" % i)    \n",
    "        print (\"The max LL for sd %.2f is %.2f\" % (i, max(ll_array)))\n",
    "        plt.axvline(x=max_val_location, color='black', ls='-.')\n",
    "        plt.legend(loc='lower left')\n",
    "\n",
    "x=np.linspace(2,10,9)\n",
    "plot_ll(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(54.)/(9. + 3.5**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, теперь $\\mu$ стало смещенно к 0, что и ожидалось.\n",
    "\n",
    "MAP-оценка выглядит следующим образом:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{\\sigma_{\\mu}^2 \\cdot \\left(\\sum x_i\\right) + \\sigma^2 \\mu_0 }{\\sigma_{\\mu}^2 n + \\sigma^2} = \\frac{1 \\cdot 54 + 0}{9 + 3.5^2} = 2.54\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Дельта-метод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Источник Casella berger стр 523, пример 10.17\n",
    "https://fsalamri.files.wordpress.com/2015/02/casella_berger_statistical_inference1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efron (1982) исследовал результаты здачи двух экзаменов LSAT(law school admission test) и GPA (grade point average). Для 15 юридических школ он получил средние значения.\n",
    "\n",
    "1. Посчитайте корреляцию LSAT и GPA\n",
    "2. Используя непараметрический бутстрап, оцените стандартное отклонение коэффициента корелляции. Используйте B=1000 ресамплов, нарисуйте гистограмму.\n",
    "3. Используйте параметрический бутстрап для оценки коэффициента корелляции. Считая что обе метрики (LSAT, GPA) представляют двумерное нормальное распределение.  Используйте B=1000 ресамплов.\n",
    "4. Если *(Х,Y)* двумерное нормальное распределение с коэффициетом корелляции *p* и где *r* - коэффициент корреляции сэмпла, тогда Дельта Метод может быть использован, для того, чтобы показать:\n",
    "\n",
    "    $$ \\sqrt{n}(r-p) \\rightarrow N\\left(0, (1-p^2 )^2\\right)$$\n",
    "    \n",
    "Сравните результаты с результатами бутстреп."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsat=[576,580,653,635,555,575,558,661,545,578,651,572,666,605,594]\n",
    "gpa=[3.39,3.07,3.12,3.30,3.00,2.74,2.81,3.43,2.76,3.03,3.36,2.88,3.44,3.13,2.96]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Посчитайте корреляцию LSAT и GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# искомые корреляции\n",
    "<YOUR_CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# склеим лист чтобы выбирать через индексы\n",
    "corrs=<YOUR_CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# корреляции через лист\n",
    "pearsonr(<YOUR_CODE>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.  Используйте параметрический бутстрап для оценки коэффициента корелляции. Считая что обе метрики (LSAT, GPA) представляют двумерное нормальное распределение.  Используйте B=1000 ресамплов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure bootstrap\n",
    "n_iterations = 1000\n",
    "n_size = 15\n",
    "idx=np.arange(0,15)\n",
    "xb = np.random.choice(idx, (n_size, n_iterations), replace=True) # бутстрепная выборка индексов из листа оценок\n",
    "#np.shape(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcorrs=np.zeros((2,n_iterations))\n",
    "\n",
    "for i in range(0,n_iterations):\n",
    "    pcorrs[:,i]=pearsonr(<YOUR_CODE>)\n",
    "print ('СТД', <YOUR_CODE>)     \n",
    "print ('Интервал 95%', np.percentile(pcorrs[0], <YOUR_CODE>))    \n",
    "\n",
    "sns.distplot(pcorrs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Используйте параметрический бутстрап для оценки коэффициента корелляции. Считая что обе метрики (LSAT, GPA) представляют двумерное нормальное распределение.  Используйте B=1000 ресамплов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.normal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx=np.mean(lsat)\n",
    "my=np.mean(gpa)\n",
    "sdx=np.std(lsat)\n",
    "sdy=np.std(gpa)\n",
    "rho=pearsonr(lsat, gpa)[0]\n",
    "b=rho*sdx/sdy\n",
    "sdxy=np.sqrt(1-rho*rho)*sdx\n",
    "rhodata=np.zeros(1000)\n",
    "rhodata[0]=rho\n",
    "for j in range(1,1000):\n",
    "    y=np.random.normal(<YOUR_CODE>)\n",
    "    x=np.random.normal(<YOUR_CODE>)\n",
    "    rhodata[j]=pearsonr(x,y)[0]\n",
    "\n",
    "print('СТД', np.std(rhodata))\n",
    "sns.distplot(rhodata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Если *(Х,Y)* двумерное нормальное распределение с коэффициетом корелляции *p* и *r* корелляция сэмпла, тогда Дельта Метод может быть использован, для того, чтобы показать:   \n",
    "$$ \\sqrt{n}(r-\\rho) \\rightarrow N\\left(0, (1-\\rho^2 )^2\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аппроксимация Дельта Метода:\n",
    "    $$ r \\sim n\\left(\\rho, (1-\\rho^2 )^2\\right/n)$$\n",
    "    \n",
    "Тогда рассчет стандартной ошибки:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кстати, график pdf этой функции будет нормальый(симметричный)\n",
    "<YOUR_CODE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Информация Фишера\n",
    "\n",
    "Информацию Фишера можно использовать для расчёта ошибок в оценке параметров и корреляций ошибок(?). \n",
    "\n",
    "Это делается из [неравенства Крамера-Рао](https://ru.wikipedia.org/wiki/%D0%9D%D0%B5%D1%80%D0%B0%D0%B2%D0%B5%D0%BD%D1%81%D1%82%D0%B2%D0%BE_%D0%9A%D1%80%D0%B0%D0%BC%D0%B5%D1%80%D0%B0_%E2%80%94_%D0%A0%D0%B0%D0%BE), которое утверждает, что снизу дисперсия параметра ограничена диагональным членом матрицы обратной к матрице Фишера.\n",
    "\n",
    "Посмотрим на оценку дисперсии параметров некоторого периодичного сигнала."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import derivative\n",
    "from scipy.optimize import curve_fit\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве сигнала мы возьмём синус со следующими параметрами: амплитуда 2, частота 8 и фаза 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal(t, a, f, ph):\n",
    "    return a * np.sin(2 * np.pi * f * t + ph)\n",
    "\n",
    "parameters = {\n",
    "\n",
    "    <YOUR_CODE>\n",
    "}\n",
    "\n",
    "noise = 0.1 # добавленный шум\n",
    "\n",
    "T = np.arange(0, 1, 0.01)\n",
    "plt.plot(T, signal(T, **parameters), '.-k')\n",
    "plt.xlabel('time (s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы посчитать границу неравенства Крамера-Рао нужно взять обратную от матрицы Фишера. В предположении что каждое измерение имеет нормальную ошибку и измерения независимы друг от друга посчитать матрицу Фишера достаточно просто.\n",
    "\n",
    "Предположим, что измерения имеют нормальное распределение и одинаковую дисперсию $\\sigma^2$, тогда матрица Фишера считается так:\n",
    "\n",
    "$$\\mathcal{I}_{mn} = \\frac{1}{\\sigma^2} \\frac{\\partial x^\\mathrm{T}}{\\partial \\theta_m} \\frac{\\partial x}{\\partial \\theta_n} = \\frac{1}{\\sigma^2} \\sum_k \\frac{\\partial x_k}{\\partial \\theta_m} \\frac{\\partial x_k}{\\partial \\theta_n}$$\n",
    "\n",
    "где $\\theta$ это вектор параметров, т.е. [амлпитуда, частота, фаза], а $x_k$ -- функция сигнала(т.е. $x(t, a, \\omega, \\phi) = a \\sin(\\omega t + \\phi)$).\n",
    "\n",
    "Переопределим: $D_{ik} := \\frac{\\partial x_k}{\\partial \\theta_i}$, тогда:\n",
    "\n",
    "$$\\mathcal{I}_{mn} = \\frac{1}{\\sigma^2} \\sum_k \\frac{\\partial x_k}{\\partial \\theta_m} \\frac{\\partial x_k}{\\partial \\theta_n} = \\frac{1}{\\sigma^2} \\sum_k D_{mk} D_{nk}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{**parameters, **{'a': 5}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.zeros((len(parameters), len(T)))\n",
    "\n",
    "# для каждого параметра\n",
    "for i, argname in enumerate(parameters.keys()):\n",
    "    # для каждого измерения\n",
    "    for k, t in enumerate(T):\n",
    "        # определим функцию по которой будем считать производную\n",
    "        func = lambda x: signal(t, **{**parameters, **{argname: x}})\n",
    "        \n",
    "        # посчитаем производную\n",
    "        D[i,k] = <YOUR_CODE># dx=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(T, signal(T, **parameters), '--k', lw=2, label='signal')\n",
    "\n",
    "for Di, argname in zip(D, parameters.keys()):\n",
    "    plt.plot(T, Di, '.-', label=argname)\n",
    "    \n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('time (s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрица $D_{ik}$ показывает как сильно $k$-ое измерение влияет на $i$-ый параметр. К примеру, видно, что на параметр амплитуды сильнее всего влияют значения в пиках синуса. \n",
    "\n",
    "Кроме того, последние точки оказываются более чувствительными к частоте. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь посчитаем матрицу Фишера по матрице $D$:\n",
    "$$\\mathcal{I}_{m,n}= \\frac{1}{\\sigma^2} \\sum_k D_{mk} D_{nk}$$\n",
    "\n",
    "Посчитаем это с помощью функции из numpy: [```einsum```](http://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.einsum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "I = <YOUR_CODE> # реализуем формулу выше по операндам ik,jk\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Граница Крамера-Рау лежит на диагонали обратной матриц от матрицы $\\mathcal{I}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_inv = <YOUR_CODE> # считаем обратную матрицу\n",
    "\n",
    "for argname, variance in zip(parameters.keys(), I_inv.diagonal()):\n",
    "    print('{}: {:.2g}'.format(argname, <YOUR_CODE>))# ошибки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = signal(T, **parameters) + np.random.randn(T.size) * noise\n",
    "plt.plot(T, S, '.-k')\n",
    "\n",
    "popt, pcov = curve_fit(signal, T, S, p0=list(parameters.values()))\n",
    "\n",
    "for argname, variance in zip(parameters.keys(), pcov.diagonal()):\n",
    "    print('{}: {:.2g}'.format(argname, np.sqrt(variance)))\n",
    "\n",
    "Tl = np.linspace(T[0], T[-1], 10000)\n",
    "plt.plot(Tl, signal(Tl, *popt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramer_rao(model, p0, X, noise, show_plot=False):\n",
    "    \"\"\"Calulate inverse of the Fisher information matrix for model\n",
    "    sampled on grid X with parameters. Assumes samples are not\n",
    "    correlated and have equal variance noise^2.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : callable\n",
    "        The model function, f(x, ...).  It must take the independent\n",
    "        variable as the first argument and the parameters as separate\n",
    "        remaining arguments.\n",
    "    X : array\n",
    "        Grid where model is sampled.\n",
    "    p0 : M-length sequence\n",
    "        Point in parameter space where Fisher information matrix is\n",
    "        evaluated.\n",
    "    noise: scalar\n",
    "        Squared variance of the noise in data.\n",
    "    show_plot : boolean\n",
    "        If True shows plot.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    iI : 2d array\n",
    "        Inverse of Fisher information matrix.\n",
    "    \"\"\"\n",
    "    D = np.zeros((len(p0), X.size))\n",
    "    for i, argname in enumerate(parameters.keys()):\n",
    "        for k, t in enumerate(T):\n",
    "            func = lambda x: signal(t, **{**parameters, **{argname: x}})\n",
    "            D[i,k] = derivative(func, parameters[argname], dx=0.0001)\n",
    "        \n",
    "    if show_plot:\n",
    "        plt.plot(X, model(X, **parameters), '--k', lw=2, label='signal')\n",
    "        for Di, argname in zip(D, parameters.keys()):\n",
    "            plt.plot(T, Di, '.-', label=argname)\n",
    "\n",
    "        plt.legend(loc='best')\n",
    "        plt.title('Parameter dependence on particular data point')\n",
    "    \n",
    "    I = np.einsum('mk,nk', D, D) / noise**2\n",
    "    I_inv = np.linalg.inv(I)\n",
    "    \n",
    "    return I_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Недиагональные элементы матрицы обратной к матрице Фишера показывают корреляции между параметрами.\n",
    "\n",
    "Посмотрим как корреляция частоты и фазы зависит от смещения оси времени. Это важно т.к. в нуле фаза фиксирована."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = []\n",
    "T0 = np.arange(-16, 6)\n",
    "for t0 in T0:\n",
    "    T = np.arange(t0, 10 + t0, 0.01)\n",
    "    print(T[0], T[-1])\n",
    "    N.append( cramer_rao(signal, parameters, T, noise)[2, 1] )\n",
    "    \n",
    "plt.plot(T0, N, 'o-')\n",
    "plt.axhline(0, color='black')\n",
    "plt.xlabel('zero location - point with fixed phase')\n",
    "plt.ylabel('frequency - phase correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.arange(-0.5, 0.51, 0.01)\n",
    "cramer_rao(signal, parameters, T, noise, show_plot=True);\n",
    "plt.xlabel('time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.arange(-1., 1, 0.01)\n",
    "cramer_rao(signal, parameters, T, noise, show_plot=True);\n",
    "plt.xlabel('time (s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Предыдущий семинар рекап (жизненная задача с jacknife)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак задача, вы периодически ловите тараканов в квартире и хотите узнать, какой же возможен самый большой таракан, учитывая что все размеры тараканов распределены равномерно. Вы не слушали этот курс, и решили что самый большой таракан из пойманных - максимально возможный; насколько вы ошиблись?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray([11,13,15,16,7,8,9,18,12,14,15,10,8,9,8,9,10,11,14,15,16,13,14,19,8,9,8,11]) # размер таракана в мм\n",
    "sns.distplot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Доказательство квадратичной точности jacknife bias estimation\n",
    "\n",
    "Для доказательства квадратичной точности оценки смещения jacknife сделаем одно простое предположение, что матожидание нашей статистики при ограниченном размере выборки раскладывается в ряд Тейлора от размера выборки:\n",
    "\n",
    "$$E_F \\hat{\\theta} = \\theta + \\frac{a_1(F)}{n-1} + \\frac{a_2(F)}{(n - 1)^2} + \\dots$$\n",
    "\n",
    "Подставим в формулу коррекции смещения jacknife оценку смещения:\n",
    "\n",
    "$$\\hat{\\theta} - \\hat{bias}_{jack} = \\hat{\\theta} - (n - 1) ( \\hat{\\theta}_{(\\cdot)} - \\hat{\\theta}) = n \\hat{\\theta} - (n - 1) \\hat{\\theta}_{(\\cdot)}$$\n",
    "\n",
    "Теперь возьмём от этого матожидание и разложим его по Тейлору:\n",
    "\n",
    "$$E_F \\left[ n \\hat{\\theta} - (n - 1) \\hat{\\theta}_{(\\cdot)} \\right] =  n\\left(\\theta + \\frac{a_1(F)}{n} + \\frac{a_2(F)}{n^2} + \\dots\\right) + $$\n",
    "\n",
    "$$ (n-1) \\left( \\theta + \\frac{a_1(F)}{n-1} + \\frac{a_2(F)}{(n-1)^2} + \\dots \\right) = $$\n",
    "\n",
    "$$ = \\theta - \\frac{a_2(F)}{n(n-1)} + o\\left(\\frac{1}{n^2}\\right)$$\n",
    "\n",
    "Как видно, jacknife избавляется от линейного члена в разложении смещения по размеру выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ещё раз определим джекнайф\n",
    "def jackknife_resampling(data):\n",
    "    n = data.shape[0]\n",
    "    resamples = np.empty([n, n-1])\n",
    "    for i in range(n):\n",
    "        resamples[i] = np.delete(data, i) # jacknife\n",
    "    return resamples\n",
    "\n",
    "\n",
    "def jackknife_stats(data, statistic):\n",
    "    from scipy.special import erfinv\n",
    "    n = data.shape[0]\n",
    "    resamples = jackknife_resampling(data)\n",
    "\n",
    "    stat_data = statistic(data)\n",
    "    jack_stat = np.apply_along_axis(statistic, 1, resamples)\n",
    "    mean_jack_stat = np.mean(jack_stat, axis=0)\n",
    "\n",
    "    bias = (n - 1) * (mean_jack_stat - stat_data)\n",
    "\n",
    "    estimate = stat_data - bias\n",
    "\n",
    "    return stat_data, bias, estimate\n",
    "\n",
    "# estimate of the variance of an estimator\n",
    "print('Пример на рандомной выборке оценки варианса метода', jackknife_stats(x, np.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Максимальный размер таракана',jackknife_stats(x, np.max)[2], 'мм')\n",
    "print('Мы бы ошиблись на', jackknife_stats(x, np.max)[2]- max(x), 'мм')\n",
    "# тут же можно напомнить про максимальную параметрическую оценку"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
